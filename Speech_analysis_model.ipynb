{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41be1d1b",
   "metadata": {},
   "source": [
    "# Подключение нужных библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5631c66",
   "metadata": {},
   "source": [
    "Первым шагом является импорт следующего списка библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f4c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ilars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ilars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ilars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ilars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score \n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#for BERT neural model\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba1f3d",
   "metadata": {},
   "source": [
    "# Загрузка нужных данных из локальных носителей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSegment.ffmpeg = \"c:/users/ilars/anaconda3/lib/site-packages/ffmpeg\"\n",
    "\n",
    "\n",
    "#Преобразовываем аудиофайлы положительных звонков в новый формат и переводим их в текстовый формат для дальнейшей обработки\n",
    "directory = \"C:\\Speach\\succ\"\n",
    "\n",
    "for filename in os.listdir(\"C:/Speach/succ\"):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    sound = AudioSegment.from_mp3(filepath)\n",
    "    sound.export(\"C:/Speach/new_succ/\"+filename[:-3]+\"wav\", format=\"wav\", parameters=[\"-ac\", \"2\", \"-ar\", \"8000\"])\n",
    "\n",
    "directory = \"C:/Speach/new_succ/\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "for filename in os.listdir(\"C:/Speach/new_succ\"):\n",
    "    r = sr.Recognizer()\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    file = sr.AudioFile(filepath)\n",
    "    with file as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        try:\n",
    "            audio = r.record(source)\n",
    "            result = r.recognize_google(audio,language=\"ru\")\n",
    "        except:\n",
    "            continue\n",
    "    texts.append({\"text\":result, \"y\":1})\n",
    "    continue\n",
    "    \n",
    "    \n",
    "#Преобразовываем аудиофайлы отрицательных звонков в новый формат и переводим их в текстовый формат для дальнейшей обработки\n",
    "directory = \"C:/Speach/non_succ/\"\n",
    "\n",
    "for filename in os.listdir(\"C:/Speach/non_succ\"):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    sound = AudioSegment.from_mp3(filepath)\n",
    "    sound.export(\"C:/Speach/new_non/\"+filename[:-3]+\"wav\", format=\"wav\", parameters=[\"-ac\", \"2\", \"-ar\", \"8000\"])\n",
    "\n",
    "directory = \"C:/Speach/new_non/\"\n",
    "\n",
    "for filename in os.listdir(\"C:/Speach/new_non\"):\n",
    "    r = sr.Recognizer()\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    file = sr.AudioFile(filepath)\n",
    "    with file as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        try:\n",
    "            audio = r.record(source)\n",
    "            result = r.recognize_google(audio,language=\"ru\")\n",
    "        except:\n",
    "            continue\n",
    "    texts.append({\"text\":result, \"y\":0})\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ecd3d",
   "metadata": {},
   "source": [
    "Далее стоит записать полученные тексты в json-формат и получить удобный для обработки датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.open(\"speach_data.json\", \"w\", encoding=\"utf-8\").write(json.dumps(texts, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b007c8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>заполненная заявка на Здравствуйте Вас приветс...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Здравствуйте Вас приветствует официальный диле...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Здравствуйте Вас приветствует Авангард официал...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Здравствуйте Вас приветствует официальный диле...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Здравствуйте Вас приветствует Авангард официал...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y\n",
       "0  заполненная заявка на Здравствуйте Вас приветс...  1\n",
       "1  Здравствуйте Вас приветствует официальный диле...  1\n",
       "2  Здравствуйте Вас приветствует Авангард официал...  1\n",
       "3  Здравствуйте Вас приветствует официальный диле...  1\n",
       "4  Здравствуйте Вас приветствует Авангард официал...  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Полученный датасет в формате pd.DataFrame\n",
    "df_train = pd.read_json('speach_data.json')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e76c57",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67a4e7",
   "metadata": {},
   "source": [
    "Прежде чем мы перейдем к построению модели, нам необходимо предварительно обработать наш набор данных, удалив знаки препинания и специальные символы, очистив тексты, удалив стоп-слова и применив лемматизацию.\n",
    "\n",
    "\n",
    "Простые процессы очистки текста: некоторые из распространенных процессов очистки текста включают:\n",
    "\n",
    "-Удаление знаков препинания, специальных символов, URL-адресов и хэштегов\n",
    "\n",
    "-Удаление начальных, завершающих и лишних пробелов / табуляций\n",
    "\n",
    "-Исправлены опечатки, сленги, аббревиатуры написаны в их длинных формах\n",
    "\n",
    "-Удаление стоп-слов: мы можем удалить список общих стоп-слов из английского словаря с помощью nltk. Несколько таких слов - \"я’, ’ты’, ’а\", \"тот\", \"он\", \"который’ и т.д.\n",
    "\n",
    "-Стемминг: относится к процессу вырезания конца или начала слов с намерением удаления аффиксов (префикс / суффикс)\n",
    "\n",
    "-Лемматизация: это процесс приведения слова к его базовой форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c947835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразовать в нижний регистр, зачеркнуть и удалить знаки препинания\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# Удаление стоп-слов\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "# Лемматизация\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# Это вспомогательная функция для сопоставления тегов положения NTLK\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Токенизация\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Получение тегов положения\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Сопоставление тегов позиции и лемматизация слов/токенов\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23deb8e1",
   "metadata": {},
   "source": [
    "В результате получаются тексты, готовые для обработки моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155cb25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>заполненная заявка на Здравствуйте Вас приветс...</td>\n",
       "      <td>1</td>\n",
       "      <td>заполненная заявка на здравствуйте вас приветс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Здравствуйте Вас приветствует официальный диле...</td>\n",
       "      <td>1</td>\n",
       "      <td>здравствуйте вас приветствует официальный диле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Здравствуйте Вас приветствует Авангард официал...</td>\n",
       "      <td>1</td>\n",
       "      <td>здравствуйте вас приветствует авангард официал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Здравствуйте Вас приветствует официальный диле...</td>\n",
       "      <td>1</td>\n",
       "      <td>здравствуйте вас приветствует официальный диле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Здравствуйте Вас приветствует Авангард официал...</td>\n",
       "      <td>1</td>\n",
       "      <td>здравствуйте вас приветствует авангард официал...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y  \\\n",
       "0  заполненная заявка на Здравствуйте Вас приветс...  1   \n",
       "1  Здравствуйте Вас приветствует официальный диле...  1   \n",
       "2  Здравствуйте Вас приветствует Авангард официал...  1   \n",
       "3  Здравствуйте Вас приветствует официальный диле...  1   \n",
       "4  Здравствуйте Вас приветствует Авангард официал...  1   \n",
       "\n",
       "                                          clean_text  \n",
       "0  заполненная заявка на здравствуйте вас приветс...  \n",
       "1  здравствуйте вас приветствует официальный диле...  \n",
       "2  здравствуйте вас приветствует авангард официал...  \n",
       "3  здравствуйте вас приветствует официальный диле...  \n",
       "4  здравствуйте вас приветствует авангард официал...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "df_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93406fdd",
   "metadata": {},
   "source": [
    "# Построение модели и обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd67dc7",
   "metadata": {},
   "source": [
    "Для упрощения работы используется кастомизированный класс Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d3b2",
   "metadata": {},
   "source": [
    "Класс CustomDataset необходим для использования с библиотекой transformers. Наследуется от класса Dataset. В нем определяются 3 обязательные функции: init, len, getitem. основное предназначение - возвращает токенизированные данные в нужном формате."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer_path = 'cointegrated/rubert-tiny'\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871165d5",
   "metadata": {},
   "source": [
    "Метод len возвращает длину нашего датасета. Метод getitem возвращает словарь, который состоит из самого исходного текста, списка токенов, маски внимания, а также метки класса. Отдельно хочется остановить на настройках токенизатора с помощью метода .encode_plus(). В этом методе мы указываем токенизатору, что исходный текст нужно обрамлять служебными токенами add_special_tokens=True, а также дополнять полученные векторы до максимально длины padding='max_len'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185276f",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bbf49",
   "metadata": {},
   "source": [
    "Задаем класс BertClassifier для работы с ним\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f19aa",
   "metadata": {},
   "source": [
    "### Инициализация:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbb99b",
   "metadata": {},
   "source": [
    "- Скачиваются модель и токенизатор из репозитория huggingface\n",
    "- Определяется наличие целевого устройства для вычислений\n",
    "- Определяется размерность ембеддингов\n",
    "- Задается количество классов\n",
    "- Задается количество эпох для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a972b0f",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Для обучения BERT нужно инициализировать несколько вспомогательных элементов:\n",
    "\n",
    "   - DataLoader: нужен для создания батчей;\n",
    "   - Optimizer: оптимизатор градиентного спуска;\n",
    "   - Scheduler: планировщик, нужен для настройки параметров оптимизатора;\n",
    "   - Loss: функция потерь, считаем по ней ошибку модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f82f1",
   "metadata": {},
   "source": [
    "### Train\n",
    "- Обучение для одной эпохи описано в методе fit.\n",
    "    - Данные в цикле батчами генерируются с помощью DataLoader;\n",
    "    - Батч подается в модель;\n",
    "    - На выходе получаем распределение вероятности по классам и значение ошибки;\n",
    "    - Делаем шаг на всех вспомогательных функциях:\n",
    "        - loss.backward: обратное распространение ошибки;\n",
    "        - clip_grad_norm: обрезаем градиенты для предотвращения \"взрыва\" градиентов;\n",
    "        - optimizer.step: шаг оптимизатора;\n",
    "        - scheduler.step: шаг планировщика;\n",
    "        - optimizer.zero_grad: обнуляем градиенты.\n",
    "- Проверку на валидационной выборке проводим с помощью метода eval. При этом используем метод torch.no_grad для предотвращения обучения на валидационной выборке.\n",
    "- Для обучения на нескольких эпохах используется метод train, в котором последовательно вызываются методы fit и eval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380812c",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Для предсказания класса для нового текста используется метод predict, который имеет смысл вызывать только после обучения модели.\n",
    "Метод работает следующим образом:\n",
    "\n",
    "- Токенизируется входной текст;\n",
    "- Токенизированный текст подается в модель;\n",
    "- На выходе получаем вероятности классов;\n",
    "- Возвращаем метку наиболее вероятного класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37972a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier:\n",
    "\n",
    "    def __init__(self, model_path, tokenizer_path, n_classes=2, epochs=1, model_save_path='bert.pt'):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_save_path=model_save_path\n",
    "        self.max_len = 512\n",
    "        self.epochs = epochs\n",
    "        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
    "        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n",
    "        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n",
    "\n",
    "        # create data loaders\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=2, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_set, batch_size=2, shuffle=True)\n",
    "\n",
    "        # helpers initialization\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=len(self.train_loader) * self.epochs\n",
    "            )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "            \n",
    "    def fit(self):\n",
    "        self.model = self.model.train()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for data in self.train_loader:\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            loss = self.loss_fn(outputs.logits, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        train_acc = correct_predictions.double() / len(self.train_set)\n",
    "        train_loss = np.mean(losses)\n",
    "        return train_acc, train_loss\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model = self.model.eval()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                    )\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                loss = self.loss_fn(outputs.logits, targets)\n",
    "                correct_predictions += torch.sum(preds == targets)\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        val_acc = correct_predictions.double() / len(self.valid_set)\n",
    "        val_loss = np.mean(losses)\n",
    "        return val_acc, val_loss\n",
    "    \n",
    "    def train(self):\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            train_acc, train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "            val_acc, val_loss = self.eval()\n",
    "            print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            if val_acc > best_accuracy:\n",
    "                torch.save(self.model, self.model_save_path)\n",
    "                best_accuracy = val_acc\n",
    "\n",
    "        self.model = torch.load(self.model_save_path)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        out = {\n",
    "              'text': text,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten()\n",
    "          }\n",
    "        \n",
    "        input_ids = out[\"input_ids\"].to(self.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8509ea5c",
   "metadata": {},
   "source": [
    "Разделим существующую обучающую выборку на тренировочный и тестовый для проверки результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"y\"],test_size=0.45,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569c0f7",
   "metadata": {},
   "source": [
    "Инициализируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BertClassifier(\n",
    "        model_path='cointegrated/rubert-tiny',\n",
    "        tokenizer_path='cointegrated/rubert-tiny',\n",
    "        n_classes=2,\n",
    "        epochs=20,\n",
    "        model_save_path='bert.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c29ec",
   "metadata": {},
   "source": [
    "Подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8366eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.preparation(\n",
    "        X_train=list(X_train),\n",
    "        y_train=list(y_train),\n",
    "        X_valid=list(X_test),\n",
    "        y_valid=list(y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2e43c",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fea1b",
   "metadata": {},
   "source": [
    "# Выгрузка модели\n",
    "Данная модель сохраняется в файл 'bert.pt'и в дальнейшем может интегрироваться в веб-приложения, программные обеспечения и т.д.\n",
    "Пример выгрузки модели написан ниже с помощью функции модуля torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cebb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertClassifier(model_path='cointegrated/rubert-tiny',tokenizer_path='cointegrated/rubert-tiny')\n",
    "model.model = torch.load('bert.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
